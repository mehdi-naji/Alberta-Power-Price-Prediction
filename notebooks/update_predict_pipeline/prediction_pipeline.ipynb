{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "import numpy as np\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.transformations.series.detrend import Deseasonalizer\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.transformations.series.boxcox import LogTransformer\n",
    "from lightgbm import LGBMRegressor\n",
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.compose import ColumnwiseTransformer\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/filtered_features_medium.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ")\n",
    "\n",
    "y_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/filtered_target_medium.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ")\n",
    "\n",
    "X_train = X_train[\"2023-04-10\":]\n",
    "y_train = y_train[\"2023-04-10\":]\n",
    "\n",
    "forecast_len = 12\n",
    "\n",
    "X_train = X_train.sort_values(by=\"date\")\n",
    "X_train = X_train.asfreq(\"H\")\n",
    "y_train = y_train.sort_values(by=\"date\")\n",
    "y_train = y_train.asfreq(\"H\")\n",
    "\n",
    "cols_for_log_transform = list(set(X_train.columns) - set(list(X_train.columns[X_train.lt(0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lgbm_forecaster():\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"price_column_transformer\",\n",
    "                ColumnwiseTransformer(LogTransformer(), columns=cols_for_log_transform),\n",
    "            ),\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\"LogTransformer\", LogTransformer()),\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                LGBMRegressor(\n",
    "                                    device=\"gpu\", num_threads=6, n_estimators=1\n",
    "                                ),\n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            )\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def initialize_lgbm_forecaster_low():\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"price_column_transformer\",\n",
    "                ColumnwiseTransformer(LogTransformer(), columns=cols_for_log_transform),\n",
    "            ),\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\"LogTransformer\", LogTransformer()),\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                LGBMRegressor(\n",
    "                                    device=\"gpu\",\n",
    "                                    num_threads=6,\n",
    "                                    n_estimators=1,\n",
    "                                    objective=\"quantile\",\n",
    "                                    alpha=0.025,\n",
    "                                ),\n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            )\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def initialize_lgbm_forecaster_high():\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"price_column_transformer\",\n",
    "                ColumnwiseTransformer(LogTransformer(), columns=cols_for_log_transform),\n",
    "            ),\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\"LogTransformer\", LogTransformer()),\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                LGBMRegressor(\n",
    "                                    device=\"gpu\",\n",
    "                                    num_threads=6,\n",
    "                                    n_estimators=1,\n",
    "                                    objective=\"quantile\",\n",
    "                                    alpha=0.975,\n",
    "                                ),\n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            )\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pipeline = initialize_lgbm_forecaster()\n",
    "lgbm_pipeline_low = initialize_lgbm_forecaster_low()\n",
    "lgbm_pipeline_high = initialize_lgbm_forecaster_high()\n",
    "fh = ForecastingHorizon(np.arange(1, forecast_len + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pipeline.fit(y=y_train, X=X_train, fh=fh)\n",
    "lgbm_pipeline_low.fit(y=y_train, X=X_train, fh=fh)\n",
    "lgbm_pipeline_high.fit(y=y_train, X=X_train, fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm_pipeline.predict(fh, X=X_train.tail(1))\n",
    "y_pred.columns = [f\"predictions\"]\n",
    "\n",
    "y_pred_lower = lgbm_pipeline_low.predict(fh, X=X_train.tail(1))\n",
    "y_pred_lower.columns = [f\"lower_bound\"]\n",
    "\n",
    "y_pred_higher = lgbm_pipeline_high.predict(fh, X=X_train.tail(1))\n",
    "y_pred_higher.columns = [f\"upper_bound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.concat([y_pred, y_pred_lower, y_pred_higher], axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"price\"] + X_train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_window = lgbm_pipeline.forecaster_.forecaster_._get_last_window()\n",
    "n_cols = len(features)\n",
    "\n",
    "window_length = 24\n",
    "y_last, X_last = lgbm_pipeline.forecaster_.forecaster_._get_last_window()\n",
    "X_pred = np.zeros((1, n_cols, window_length))\n",
    "\n",
    "X_pred[:, 0, :] = y_last\n",
    "X_pred[:, 1:, :] = X_last.T\n",
    "X_pred = X_pred.reshape(1, -1)\n",
    "\n",
    "X_pred = X_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feature_list = []\n",
    "\n",
    "for feature in features:\n",
    "    for i in range(24, 0, -1):\n",
    "        reduced_feature_list.append(f'{feature}_lag{i}')\n",
    "        \n",
    "X_reduced_df = pd.DataFrame(\n",
    "    [X_pred],\n",
    "    columns=reduced_feature_list\n",
    ").round(3)\n",
    "\n",
    "X_reduced_df.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_models = lgbm_pipeline.forecaster_.forecaster_.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_df = pd.DataFrame([], columns=reduced_feature_list)\n",
    "\n",
    "# Lists for storing base values: Average of predications made using X_train\n",
    "base_list = []\n",
    "base_transformed = [] \n",
    "\n",
    "for model in lgbm_models:\n",
    "    explainerModel = shap.TreeExplainer(model)\n",
    "    shap_values = explainerModel.shap_values(X_reduced_df).flatten()\n",
    "    shap_df.loc[len(shap_df)] = shap_values\n",
    "    base_list.append(lgbm_pipeline.forecaster_._get_inverse_transform(lgbm_pipeline.forecaster_.transformers_pre_, np.array([explainerModel.expected_value]))[0])\n",
    "    base_transformed.append(explainerModel.expected_value)\n",
    "    \n",
    "\n",
    "shap_df['date'] = y_pred.index\n",
    "shap_df['base_val'] = base_list\n",
    "shap_df['base_val_transformed'] = base_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_df = pd.DataFrame(columns=['Date', 'Feature', 'Value','Base','Base_transformed'])\n",
    "# Iterate over each row\n",
    "for index, row in shap_df.iterrows():\n",
    "    date = row['date']\n",
    "    base = row['base_val']\n",
    "    base_transformed = row['base_val_transformed']\n",
    "    # Drop the 'Date' column for sorting\n",
    "    sorted_row = row.drop(['date','base_val','base_val_transformed']).sort_values(ascending=False)\n",
    "    \n",
    "    # Select top 4 features and values\n",
    "    top_features = sorted_row.iloc[:4]\n",
    "    \n",
    "    # Calculate the sum of the remaining features\n",
    "    other_value = sorted_row.iloc[4:].sum()\n",
    "    \n",
    "    # Create a DataFrame for top features\n",
    "    temp_df = pd.DataFrame({'Date': [date] * 4,\n",
    "                            'Feature': top_features.index,\n",
    "                            'Value': top_features.values,\n",
    "                           'Base': [base] * 4,\n",
    "                           'Base_transformed': [base_transformed] * 4})\n",
    "    \n",
    "    # Append 'other' feature\n",
    "    other_df = pd.DataFrame({'Date': [date],\n",
    "                             'Feature': ['other'],\n",
    "                             'Value': [other_value],\n",
    "                            'Base': [base],\n",
    "                            'Base_transformed': [base_transformed]})\n",
    "    \n",
    "    # Concatenate top features and 'other' feature\n",
    "    temp_df = pd.concat([temp_df, other_df], ignore_index=True)\n",
    "    \n",
    "    # Append to the new DataFrame\n",
    "    top_features_df = pd.concat([top_features_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Rename the columns\n",
    "top_features_df.columns = ['Date', 'Feature', 'Value','Base','Base_transformed']\n",
    "\n",
    "top_features_df['Value'] = top_features_df['Value']/top_features_df['Base_transformed']\n",
    "top_features_df = top_features_df.drop('Base_transformed', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original date of the first row\n",
    "original_date = top_features_df.iloc[0]['Date']\n",
    "\n",
    "# Calculate the start date for duplication (12 hours before the original date)\n",
    "start_date = original_date - pd.DateOffset(hours=12)\n",
    "\n",
    "# Create a list of dates for duplication\n",
    "dates = pd.date_range(start=start_date, periods=12, freq='H')\n",
    "\n",
    "# Duplicate the first 5 rows for each date\n",
    "duplicated_rows = pd.concat([top_features_df.iloc[:5].assign(Date=date) for date in dates])\n",
    "\n",
    "# Reset the index of the duplicated rows\n",
    "duplicated_rows.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_df = pd.concat([top_features_df, duplicated_rows])\n",
    "final_feature_df.reset_index(drop=True, inplace=True)\n",
    "final_feature_df.sort_values(by=['Date'], inplace=True)\n",
    "final_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite to the hive\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# df = spark.createDataFrame(final_feature_df)\n",
    "# df = df.withColumn(\"Base\", df[\"Base\"].cast('long'))\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_dataframe(df):\n",
    "    # Convert 'Date' column to datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Group the DataFrame by date\n",
    "    grouped = df.groupby('Date')\n",
    "\n",
    "    # Create an empty DataFrame to store the generated sentences\n",
    "    result_df = pd.DataFrame(columns=['Date', 'Sentence'])\n",
    "\n",
    "    # Iterate over each date and generate the sentence\n",
    "    for date, group in grouped:\n",
    "        # Get the base value for the date\n",
    "        base_value = group['Base'].iloc[0]\n",
    "\n",
    "        # Create the sentence\n",
    "        sentence = f\"The average power price of the past month is ${base_value:.1f}. The top two variables that impact the prediction are:\"\n",
    "\n",
    "        # Sort the group by descending absolute values to get the top two variables\n",
    "        sorted_group = group[group['Feature'] != 'other'].sort_values('Value', key=abs, ascending=False).head(2)\n",
    "\n",
    "        # Iterate over the sorted group\n",
    "        for _, row in sorted_group.iterrows():\n",
    "            feature = row['Feature']\n",
    "            value = row['Value']\n",
    "\n",
    "            # Determine whether the value is an increase or decrease\n",
    "            if value < 0:\n",
    "                change_type = \"decreases\"\n",
    "            else:\n",
    "                change_type = \"increases\"\n",
    "\n",
    "            # Calculate the absolute value of the change and format as a percentage with one decimal place\n",
    "            abs_value = abs(value)\n",
    "            change_percentage = abs_value * 100\n",
    "\n",
    "            # Add the feature information to the sentence\n",
    "            sentence += f\" {feature} {change_type} the prediction by {change_percentage:.3f}%,\"\n",
    "        \n",
    "        # Remove the trailing comma and add a period at the end of the sentence\n",
    "        sentence = sentence.rstrip(\",\") + \".\"\n",
    "\n",
    "        # Create a temporary DataFrame for the current date and sentence\n",
    "        temp_df = pd.DataFrame({'Date': [date], 'Sentence': [sentence]})\n",
    "\n",
    "        # Append the temporary DataFrame to the result DataFrame\n",
    "        result_df = pd.concat([result_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Sort the result DataFrame by date in ascending order\n",
    "    result_df = result_df.sort_values('Date', ascending=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "generate_sentence_dataframe(final_feature_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain_feature_importance = generate_sentence_dataframe(final_feature_df)\n",
    "# df_shap_explain = spark.createDataFrame(explain_feature_importance)\n",
    "# df_shap_explain.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/shap_explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_prediction = y_pred.rename(columns={y_pred.columns.tolist()[0]: 'price'})\n",
    "price_prediction = price_prediction.reset_index()\n",
    "price_prediction = price_prediction.rename(columns={'index': 'date'})\n",
    "price_prediction_upper = copy.deepcopy(price_prediction)\n",
    "price_prediction_upper['indicator'] = 'past_predicted'\n",
    "price_prediction_upper['upper_bound'] = np.nan\n",
    "price_prediction_upper['lower_bound'] = np.nan\n",
    "price_prediction_lower = copy.deepcopy(price_prediction)\n",
    "price_prediction_lower['indicator'] = np.nan\n",
    "price_prediction_lower['price'] = np.nan\n",
    "price_prediction = pd.concat([price_prediction_upper, price_prediction_lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_prediction['date'] = pd.to_datetime(price_prediction['date'])\n",
    "first_row_date = price_prediction['date'].iloc[0]\n",
    "start_time = first_row_date - pd.Timedelta(hours=12)\n",
    "price_past = pd.DataFrame({\n",
    "    'date': pd.date_range(start=start_time, periods=12, freq='H'),\n",
    "    'price': [None] * 12,\n",
    "    'upper_bound': [None] * 12,\n",
    "    'lower_bound': [None] * 12,\n",
    "    'indicator': ['past_predicted'] * 12,\n",
    "    'price': [None] * 12\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_date = (first_row_date - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Calculate the end date by adding one day to current_date\n",
    "end_date = (first_row_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "url = 'https://api.aeso.ca/report/v1.1/price/poolPrice'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'X-API-Key': 'eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ6MHo4MnIiLCJpYXQiOjE2ODM1NzQyMTh9.Gbod9kjeDwP4SOJibSFof63X7GGZxbZdBmBVrgE409w'\n",
    "}\n",
    "\n",
    "# Set the start date as a parameter\n",
    "params = {\n",
    "    'startDate': start_date,\n",
    "    'endDate': end_date\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "api_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data = pd.DataFrame(api_data['return']['Pool Price Report'])\n",
    "api_data = api_data.drop(['begin_datetime_utc', 'forecast_pool_price', 'rolling_30day_avg'], axis=1)\n",
    "price_past['date'] = pd.to_datetime(price_past['date'])\n",
    "api_data['begin_datetime_mpt'] = pd.to_datetime(api_data['begin_datetime_mpt'])\n",
    "\n",
    "# Perform the join operation based on the matching columns\n",
    "joined_df = pd.merge(price_past, api_data, left_on='date', right_on='begin_datetime_mpt')\n",
    "\n",
    "# Replace the null values in price_past with the corresponding values from api_data\n",
    "joined_df['price'] = joined_df['price'].fillna(joined_df['pool_price'])\n",
    "\n",
    "joined_df = joined_df.drop(['begin_datetime_mpt', 'pool_price'], axis=1)\n",
    "input_features = pd.read_csv('https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/filtered_features_medium.csv')\n",
    "input_features = input_features[['date','wind_supply_mix','wind_reserve_margin','gas_supply_mix','load_on_gas_reserve']]\n",
    "input_features['date'] = pd.to_datetime(input_features['date'])\n",
    "joined_df = pd.merge(joined_df, input_features, left_on='date', right_on='date') \n",
    "predicted_price = pd.concat([joined_df,price_prediction], axis=0, ignore_index=True)\n",
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predicted_price = spark.createDataFrame(predicted_price)\n",
    "# df_predicted_price = df_predicted_price.withColumn(\"price\", df_predicted_price[\"price\"].cast('Double'))\n",
    "# df_predicted_price.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/user/hive/warehouse/predicted_price\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slalomenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
