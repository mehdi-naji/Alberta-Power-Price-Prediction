{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime_custom_reduce import make_reduction\n",
    "from lightgbm import LGBMRegressor\n",
    "from sktime_custom_pipeline import ForecastingPipeline, TransformedTargetForecaster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to run hyperparameter optimization for the model\n",
    "\n",
    "As the ForecastingGridSearchCV and ForecastingRandomizedSearchCV of sktime are not capable of utilizing the warm initialization feature of LightGBM, we have to implement our own hyperparameter optimization. \n",
    "\n",
    "We're relying on an expanding window approach here. We consider the initial training window length as Jan 1st 2021 to Jan 31st 2021. We then expand the training window by 12 hours and retrain the model. We repeat this process until we reach the end of the training data. We'll try out different hyperparameter combinations for each training window and evaluate the performance on the validation set. The best performing hyperparameter combination will be used for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/train/X_train.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ")\n",
    "\n",
    "y_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/train/y_train.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ")\n",
    "\n",
    "X_train = X_train.sort_values(by=\"date\")\n",
    "X_train = X_train.asfreq(\"H\")\n",
    "y_train = y_train.sort_values(by=\"date\")\n",
    "y_train = y_train.asfreq(\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train[\"2023-01-01\":]\n",
    "y_test = y_train[\"2023-01-01\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:\"2022-12-31\"]\n",
    "y_train = y_train[:\"2022-12-31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lgbm_forecaster(boosting_type, learning_rate, max_depth, num_leaves=None, reg_alpha=None, reg_lambda=None, min_data_in_leaf=None):\n",
    "    regressor = LGBMRegressor(\n",
    "        device=\"gpu\",\n",
    "        n_jobs=-1,\n",
    "        n_estimators=1000,\n",
    "        boosting_type=boosting_type,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "\n",
    "    if num_leaves is not None:\n",
    "        regressor.num_leaves = num_leaves\n",
    "    if reg_alpha is not None:\n",
    "        regressor.reg_alpha = reg_alpha\n",
    "    if reg_lambda is not None:\n",
    "        regressor.reg_lambda = reg_lambda\n",
    "    if min_data_in_leaf is not None:\n",
    "        regressor.min_data_in_leaf = min_data_in_leaf\n",
    "\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                regressor,\n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "# initialize pipelines with hyperparameters\n",
    "pipelines = [\n",
    "    initialize_lgbm_forecaster(\"gbdt\", 0.01, 20),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.01, 20),\n",
    "    initialize_lgbm_forecaster(\"gbdt\", 0.08, 50, num_leaves=31, reg_alpha=10, reg_lambda=30, min_data_in_leaf=20),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.01, 100, num_leaves=70, reg_alpha=30, reg_lambda=10, min_data_in_leaf=30),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.15, 150, num_leaves=80, reg_alpha=10, reg_lambda=20, min_data_in_leaf=40),\n",
    "    initialize_lgbm_forecaster(\"gbdt\", 0.08, -1, num_leaves=31, reg_alpha=20, reg_lambda=10, min_data_in_leaf=30),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.01, 15, num_leaves=70, reg_alpha=30, reg_lambda=20, min_data_in_leaf=20),\n",
    "    initialize_lgbm_forecaster(\"gbdt\", 0.15, 25, num_leaves=80, reg_alpha=10, reg_lambda=30, min_data_in_leaf=40),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.08, 40, num_leaves=31, reg_alpha=20, reg_lambda=10, min_data_in_leaf=20),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.01, -1, num_leaves=70, reg_alpha=10, reg_lambda=20, min_data_in_leaf=30),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.15, 50, num_leaves=80, reg_alpha=30, reg_lambda=30, min_data_in_leaf=40),\n",
    "    initialize_lgbm_forecaster(\"gbdt\", 0.08, 100, num_leaves=31, reg_alpha=20, reg_lambda=10, min_data_in_leaf=20),\n",
    "]\n",
    "\n",
    "fh = ForecastingHorizon(np.arange(1, 12 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline 1...\n",
      "Average RMSE for each fold: 95.72728987761006\n",
      "Training pipeline 2...\n",
      "Average RMSE for each fold: 89.29048801316956\n",
      "Training pipeline 3...\n",
      "Average RMSE for each fold: 99.46342427422609\n",
      "Training pipeline 4...\n",
      "Average RMSE for each fold: 90.59810093166509\n",
      "Training pipeline 5...\n",
      "Average RMSE for each fold: 96.93036342804002\n",
      "Training pipeline 6...\n",
      "Average RMSE for each fold: 98.0828910076149\n",
      "Training pipeline 7...\n",
      "Average RMSE for each fold: 89.72298936169489\n",
      "Training pipeline 8...\n",
      "Average RMSE for each fold: 98.4681823243143\n",
      "Training pipeline 9...\n",
      "Average RMSE for each fold: 95.44288457500416\n",
      "Training pipeline 10...\n",
      "Average RMSE for each fold: 90.0493276150256\n",
      "Training pipeline 11...\n",
      "Average RMSE for each fold: 99.01450682378139\n",
      "Training pipeline 12...\n",
      "Average RMSE for each fold: 98.07196136334963\n"
     ]
    }
   ],
   "source": [
    "pipeline_assets = []\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    \n",
    "    print(f\"Training pipeline {i+1}...\")\n",
    "    \n",
    "    rolling_prediction_df = pd.DataFrame()\n",
    "    pipeline.fit(y=y_train, X=X_train, fh=fh)\n",
    "\n",
    "    y_pred = pipeline.predict(fh, X=X_train.tail(1))\n",
    "    y_pred.columns = [f\"cutoff_hour_{pipeline.cutoff.hour[0]}\"]\n",
    "    rolling_prediction_df = pd.concat([rolling_prediction_df, y_pred], axis=1)\n",
    "      \n",
    "      \n",
    "    for i in range(0, len(y_test), 12):\n",
    "\n",
    "            new_observation_y, new_observation_X  = y_test[i:i+12], X_test[i:i+12]\n",
    "            \n",
    "            new_observation_y = new_observation_y.asfreq('H')\n",
    "            new_observation_X = new_observation_X.asfreq('H')\n",
    "\n",
    "            pipeline.update(y=new_observation_y, X=new_observation_X, update_params=True)\n",
    "\n",
    "            pipeline.cutoff.freq = 'H'\n",
    "\n",
    "            cutoff_time = pipeline.cutoff\n",
    "            prediction_for = cutoff_time + pd.DateOffset(hours=i)\n",
    "\n",
    "            y_pred = pipeline.predict(fh, X=new_observation_X)\n",
    "            \n",
    "            y_pred.columns = [f\"cutoff_hour_{pipeline.cutoff.hour[0]}\"]\n",
    "            \n",
    "            rolling_prediction_df = pd.concat([rolling_prediction_df, y_pred], axis=1)\n",
    "            \n",
    "    rmse_list = []\n",
    "    fold_actuals = []\n",
    "    fold_predictions_list = []\n",
    "\n",
    "    for col in range(rolling_prediction_df.shape[1]-1):\n",
    "        \n",
    "        fold_predictions = rolling_prediction_df.iloc[:, col].dropna()\n",
    "        \n",
    "        fold_indices = fold_predictions.index  \n",
    "\n",
    "        y_test_subset = y_test.loc[fold_indices]  \n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test_subset, fold_predictions))  \n",
    "        \n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "        fold_actuals.append(y_test_subset)\n",
    "        fold_predictions_list.append(fold_predictions)\n",
    "\n",
    "    print(f\"Average RMSE for each fold: {np.mean(rmse_list)}\")\n",
    "\n",
    "    asset_dict = {\"actuals\": fold_actuals, \"predictions\": fold_predictions_list, \"rmse\": rmse_list, \"pipeline\": pipeline, \"rolling_prediction_df\": rolling_prediction_df}\n",
    "\n",
    "    pipeline_assets.append(asset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the lowest average RMSE is: ForecastingPipeline(steps=[('forecaster',\n",
      "                            TransformedTargetForecaster(steps=[('forecast',\n",
      "                                                                DirectTabularRegressionForecaster(estimator=LGBMRegressor(boosting_type='dart', device='gpu', learning_rate=0.01,\n",
      "              max_depth=20, n_estimators=1000),\n",
      "                                                                                                  window_length=24))]))])\n"
     ]
    }
   ],
   "source": [
    "lowest_average_rmse = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Iterating over the list of dictionaries\n",
    "for asset_dict in pipeline_assets:\n",
    "    avg_rmse = sum(asset_dict[\"rmse\"]) / len(asset_dict[\"rmse\"])  # Calculate the average RMSE\n",
    "    \n",
    "    # If this model has a lower average RMSE than the current best model, update the best model and lowest RMSE\n",
    "    if avg_rmse < lowest_average_rmse:\n",
    "        lowest_average_rmse = avg_rmse\n",
    "        best_model = asset_dict\n",
    "\n",
    "print(f'The model with the lowest average RMSE is: {best_model[\"pipeline\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tuning_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline_assets, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slalomenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
