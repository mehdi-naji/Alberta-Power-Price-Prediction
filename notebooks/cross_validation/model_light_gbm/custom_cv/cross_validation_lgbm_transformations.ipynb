{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "- **LightGBM**\n",
    "- **63 Folds (~ 1 month)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sktime_custom_pipeline import ForecastingPipeline, TransformedTargetForecaster\n",
    "from sktime_custom_reduce import make_reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to run hyperparameter optimization for the model\n",
    "\n",
    "As the ForecastingGridSearchCV and ForecastingRandomizedSearchCV of sktime are not capable of utilizing the warm initialization feature of LightGBM, we have to implement our own hyperparameter optimization. \n",
    "\n",
    "We're relying on an expanding window approach here. We consider the initial training window length as Jan 1st 2021 to Jan 31st 2021. We then expand the training window by 12 hours and retrain the model. We repeat this process until we reach the end of the training data. We'll try out different hyperparameter combinations for each training window and evaluate the performance on the validation set. The best performing hyperparameter combination will be used for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the end date of the training data. The rest of the code will automatically create the necessary validation folds. \n",
    "# The results will be saved as a pickle file in the same folder as this notebook. It can also been seen in the output of the notebook.\n",
    "train_end = \"2022-12-31\"\n",
    "create_validation_from = \"2023-01-01\"\n",
    "device = \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/train/X_train.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ")\n",
    "\n",
    "y_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/slalom-ubc-mds/Power-Price-Prediction/main/data/processed/train/y_train.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ")\n",
    "\n",
    "X_train = X_train.sort_values(by=\"date\")\n",
    "X_train = X_train.asfreq(\"H\")\n",
    "y_train = y_train.sort_values(by=\"date\")\n",
    "y_train = y_train.asfreq(\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train[create_validation_from:]\n",
    "y_test = y_train[create_validation_from:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:train_end]\n",
    "y_train = y_train[:train_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_log_transform = list(set(X_train.columns) - set(list(X_train.columns[X_train.lt(5).any()])) - set([\"weekly_profile\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sktime.transformations.compose import ColumnwiseTransformer\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sktime.transformations.series.boxcox import LogTransformer\n",
    "\n",
    "def initialize_log_std_lgbm_forecaster(boosting_type, learning_rate, max_depth, num_leaves=None, reg_alpha=None, reg_lambda=None, min_data_in_leaf=None):\n",
    "    regressor = LGBMRegressor(\n",
    "        device=device,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=1000,\n",
    "        boosting_type=boosting_type,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "\n",
    "    if num_leaves is not None:\n",
    "        regressor.num_leaves = num_leaves\n",
    "    if reg_alpha is not None:\n",
    "        regressor.reg_alpha = reg_alpha\n",
    "    if reg_lambda is not None:\n",
    "        regressor.reg_lambda = reg_lambda\n",
    "    if min_data_in_leaf is not None:\n",
    "        regressor.min_data_in_leaf = min_data_in_leaf\n",
    "\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\"log_column_transformer\", ColumnwiseTransformer(LogTransformer(), columns=cols_for_log_transform)),\n",
    "            (\"std_column_transformer\", TabularToSeriesAdaptor(StandardScaler())),\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\"log_column_transformer\", LogTransformer()),\n",
    "                        (\"std_column_transformer\", TabularToSeriesAdaptor(StandardScaler())),\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                regressor,  \n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def initialize_log_lgbm_forecaster(boosting_type, learning_rate, max_depth, num_leaves=None, reg_alpha=None, reg_lambda=None, min_data_in_leaf=None):\n",
    "    regressor = LGBMRegressor(\n",
    "        device=device,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=1000,\n",
    "        boosting_type=boosting_type,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "\n",
    "    if num_leaves is not None:\n",
    "        regressor.num_leaves = num_leaves\n",
    "    if reg_alpha is not None:\n",
    "        regressor.reg_alpha = reg_alpha\n",
    "    if reg_lambda is not None:\n",
    "        regressor.reg_lambda = reg_lambda\n",
    "    if min_data_in_leaf is not None:\n",
    "        regressor.min_data_in_leaf = min_data_in_leaf\n",
    "\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\"log_column_transformer\", ColumnwiseTransformer(LogTransformer(), columns=cols_for_log_transform)),\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\"log_column_transformer\", LogTransformer()),\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                regressor,  \n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def initialize_lgbm_forecaster(boosting_type, learning_rate, max_depth, num_leaves=None, reg_alpha=None, reg_lambda=None, min_data_in_leaf=None):\n",
    "    regressor = LGBMRegressor(\n",
    "        device=device,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=1000,\n",
    "        boosting_type=boosting_type,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "\n",
    "    if num_leaves is not None:\n",
    "        regressor.num_leaves = num_leaves\n",
    "    if reg_alpha is not None:\n",
    "        regressor.reg_alpha = reg_alpha\n",
    "    if reg_lambda is not None:\n",
    "        regressor.reg_lambda = reg_lambda\n",
    "    if min_data_in_leaf is not None:\n",
    "        regressor.min_data_in_leaf = min_data_in_leaf\n",
    "\n",
    "    pipe = ForecastingPipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"forecaster\",\n",
    "                TransformedTargetForecaster(\n",
    "                    [\n",
    "                        (\n",
    "                            \"forecast\",\n",
    "                            make_reduction(\n",
    "                                regressor,\n",
    "                                window_length=24,\n",
    "                                strategy=\"direct\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "fh = ForecastingHorizon(np.arange(1, 12 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pipelines with hyperparameters\n",
    "pipelines = [\n",
    "    initialize_log_lgbm_forecaster(\"gbdt\", 0.01, 20),\n",
    "    initialize_log_lgbm_forecaster(\"dart\", 0.01, 20),\n",
    "    initialize_log_std_lgbm_forecaster(\"gbdt\", 0.01, 20),\n",
    "    initialize_log_std_lgbm_forecaster(\"dart\", 0.01, 20),\n",
    "    initialize_lgbm_forecaster(\"dart\", 0.01, 20),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline 1...\n",
      "Average RMSE for each fold: 85.588833878514\n",
      "STD RMSE for each fold: 83.14453898529717\n",
      "MIN RMSE for each fold: 8.55774434890682\n",
      "MAX RMSE for each fold: 350.1433484136448\n",
      "Training pipeline 2...\n",
      "Average RMSE for each fold: 103.78493800112403\n",
      "STD RMSE for each fold: 94.08296945130428\n",
      "MIN RMSE for each fold: 14.465232869183872\n",
      "MAX RMSE for each fold: 401.4911092193406\n",
      "Training pipeline 3...\n",
      "Average RMSE for each fold: 86.15834783115243\n",
      "STD RMSE for each fold: 83.16394163647337\n",
      "MIN RMSE for each fold: 8.834967190727612\n",
      "MAX RMSE for each fold: 352.8045049505404\n",
      "Training pipeline 4...\n",
      "Average RMSE for each fold: 85.73208521996806\n",
      "STD RMSE for each fold: 85.97135835862902\n",
      "MIN RMSE for each fold: 9.111699204323294\n",
      "MAX RMSE for each fold: 360.04744485556625\n",
      "Training pipeline 5...\n",
      "Average RMSE for each fold: 89.39899806519014\n",
      "STD RMSE for each fold: 79.79654334071667\n",
      "MIN RMSE for each fold: 9.606605968497064\n",
      "MAX RMSE for each fold: 344.27675614094903\n"
     ]
    }
   ],
   "source": [
    "pipeline_assets = []\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    \n",
    "    print(f\"Training pipeline {i+1}...\")\n",
    "    \n",
    "    rolling_prediction_df = pd.DataFrame()\n",
    "    pipeline.fit(y=y_train, X=X_train, fh=fh)\n",
    "\n",
    "    y_pred = pipeline.predict(fh, X=X_train.tail(1))\n",
    "    y_pred.columns = [f\"cutoff_hour_{pipeline.cutoff.hour[0]}\"]\n",
    "    rolling_prediction_df = pd.concat([rolling_prediction_df, y_pred], axis=1)\n",
    "      \n",
    "      \n",
    "    for i in range(0, len(y_test), 12):\n",
    "\n",
    "            new_observation_y, new_observation_X  = y_test[i:i+12], X_test[i:i+12]\n",
    "            \n",
    "            new_observation_y = new_observation_y.asfreq('H')\n",
    "            new_observation_X = new_observation_X.asfreq('H')\n",
    "\n",
    "            pipeline.update(y=new_observation_y, X=new_observation_X, update_params=True)\n",
    "\n",
    "            pipeline.cutoff.freq = 'H'\n",
    "\n",
    "            cutoff_time = pipeline.cutoff\n",
    "            prediction_for = cutoff_time + pd.DateOffset(hours=i)\n",
    "\n",
    "            y_pred = pipeline.predict(fh, X=new_observation_X)\n",
    "            \n",
    "            y_pred.columns = [f\"cutoff_hour_{pipeline.cutoff.hour[0]}\"]\n",
    "            \n",
    "            rolling_prediction_df = pd.concat([rolling_prediction_df, y_pred], axis=1)\n",
    "            \n",
    "    rmse_list = []\n",
    "    fold_actuals = []\n",
    "    fold_predictions_list = []\n",
    "\n",
    "    for col in range(rolling_prediction_df.shape[1]-1):\n",
    "        \n",
    "        fold_predictions = rolling_prediction_df.iloc[:, col].dropna()\n",
    "        \n",
    "        fold_indices = fold_predictions.index  \n",
    "\n",
    "        y_test_subset = y_test.loc[fold_indices]  \n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test_subset, fold_predictions))  \n",
    "        \n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "        fold_actuals.append(y_test_subset)\n",
    "        fold_predictions_list.append(fold_predictions)\n",
    "\n",
    "    print(f\"Average RMSE for each fold: {np.mean(rmse_list)}\")\n",
    "    print(f\"STD RMSE for each fold: {np.std(rmse_list)}\")\n",
    "    print(f\"MIN RMSE for each fold: {np.min(rmse_list)}\")\n",
    "    print(f\"MAX RMSE for each fold: {np.max(rmse_list)}\")\n",
    "\n",
    "    asset_dict = {\"actuals\": fold_actuals, \"predictions\": fold_predictions_list, \"rmse\": rmse_list, \"pipeline\": pipeline}\n",
    "\n",
    "    pipeline_assets.append(asset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the lowest average RMSE is: ForecastingPipeline(steps=[('log_column_transformer',\n",
      "                            ColumnwiseTransformer(columns=['demand_supply_ratio',\n",
      "                                                           'gas_tng',\n",
      "                                                           'calgary_load',\n",
      "                                                           'rolling_mean',\n",
      "                                                           'total_reserve_margin',\n",
      "                                                           'hydro_tng',\n",
      "                                                           'hydro_reserve_margin',\n",
      "                                                           'northwest_load',\n",
      "                                                           'exp_moving_avg',\n",
      "                                                           'rolling_median',\n",
      "                                                           'rolling_max',\n",
      "                                                           'system_load',\n",
      "                                                           'gas_supply_mix',\n",
      "                                                           'fossil_fuel_ratio'],\n",
      "                                                  transformer=LogTransformer())),\n",
      "                           ('forecaster',\n",
      "                            TransformedTargetForecaster(steps=[('log_column_transformer',\n",
      "                                                                LogTransformer()),\n",
      "                                                               ('forecast',\n",
      "                                                                DirectTabularRegressionForecaster(estimator=LGBMRegressor(device='gpu', learning_rate=0.01, max_depth=20, n_estimators=1000),\n",
      "                                                                                                  window_length=24))]))])\n"
     ]
    }
   ],
   "source": [
    "lowest_average_rmse = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Iterating over the list of dictionaries\n",
    "for asset_dict in pipeline_assets:\n",
    "    avg_rmse = sum(asset_dict[\"rmse\"]) / len(asset_dict[\"rmse\"])  # Calculate the average RMSE\n",
    "    \n",
    "    # If this model has a lower average RMSE than the current best model, update the best model and lowest RMSE\n",
    "    if avg_rmse < lowest_average_rmse:\n",
    "        lowest_average_rmse = avg_rmse\n",
    "        best_model = asset_dict\n",
    "\n",
    "print(f'The model with the lowest average RMSE is: {best_model[\"pipeline\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tuning_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline_assets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv_results = []\n",
    "rmse_cv_std = []\n",
    "rmse_cv_min = []\n",
    "rmse_cv_max = []\n",
    "rmse = np.mean(pipeline_assets[4]['rmse'])\n",
    "rmse_std = np.std(pipeline_assets[4]['rmse'])\n",
    "rmse_min = np.min(pipeline_assets[4]['rmse'])\n",
    "rmse_max = np.max(pipeline_assets[4]['rmse'])\n",
    "rmse_cv_results.append(rmse)\n",
    "rmse_cv_std.append(rmse_std)\n",
    "rmse_cv_min.append(rmse_min)\n",
    "rmse_cv_max.append(rmse_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE_CV</th>\n",
       "      <th>RMSE_CV_STD</th>\n",
       "      <th>RMSE_MIN</th>\n",
       "      <th>RMSE_MAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>89.398998</td>\n",
       "      <td>79.796543</td>\n",
       "      <td>9.606606</td>\n",
       "      <td>344.276756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model    RMSE_CV  RMSE_CV_STD  RMSE_MIN    RMSE_MAX\n",
       "0  LGBM  89.398998    79.796543  9.606606  344.276756"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 step prediction errors\n",
    "rmse_cv_results_df = pd.DataFrame(\n",
    "    {\"Model\": \"LGBM\", \"RMSE_CV\": rmse_cv_results, \"RMSE_CV_STD\": rmse_cv_std, \"RMSE_MIN\": rmse_cv_min, \"RMSE_MAX\": rmse_cv_max}\n",
    ").sort_values(by=[\"RMSE_CV\"])\n",
    "\n",
    "rmse_cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_lgbm_results.pkl', 'wb') as f:\n",
    "    pickle.dump(rmse_cv_results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slalomenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
